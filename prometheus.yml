global:
  scrape_interval:     1m
  evaluation_interval: 1m
  external_labels:
    monitor: 'pi-farm'

scrape_configs:
  # Make Prometheus scrape itself for metrics.
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Job for Docker Swarm containers.
#  - job_name: 'dockerswarm'
#    dockerswarm_sd_configs:
#      - host: unix:///var/run/docker.sock
#        role: tasks
#    relabel_configs:
#      # Only keep containers that should be running.
#      - source_labels: [__meta_dockerswarm_task_desired_state]
#        regex: running
#        action: keep

  # Job for Docker containers.
  - job_name: "docker"
    static_configs:
      - targets: ['192.168.1.32:9323', '192.168.1.34:9323', '192.168.1.36:9323', '192.168.1.38:9323', '192.168.1.40:9323', '192.168.1.42:9323']

  # Job for guinea-bot should reference container, not host, in swarm mode.
  - job_name: 'guinea_bot'
    metrics_path: /
    static_configs:
      - targets: ['192.168.1.38:8000']

  # Job for portfolio-template should reference container, not host, in swarm mode.
  - job_name: 'portfolio_template'
    static_configs:
      - targets: ['192.168.1.42:3000']

  # Job for portfolio-map should reference container, not host, in swarm mode.
  - job_name: 'portfolio_map'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['192.168.1.42:8001']

  # Job for portfolio-sql should reference container, not host, in swarm mode.
  - job_name: 'portfolio_sql'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['192.168.1.42:8002']
